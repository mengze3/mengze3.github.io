---
layout: archive
title: "Transformer
-
based Trajectory Planning for Robots: Extending to UAVs and UGVs"
permalink: /car/
author_profile: false
redirect_from:
  - /resume
---

{% include base_path %}

<head>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <style>
        .container {
            display: flex; /* Use Flexbox for layout */
            align-items: start; /* Align items at the start of the container */
        }
        .video {
            width: 50%; /* Assign 50% width for the video */
            padding-right: 10px; /* Add some space between video and text */
        }
        .text {
            width: 50%; /* Assign 50% width for the text */
        }
      .center {
            text-align: center;
        }
        iframe {
            width: 100%; /* Make the video responsive */
            height: 300px; /* Set a fixed height for the video */
        }
        .flex-container {
            display: flex;
        }
    </style>
</head>


<body>


    <h1>Contributions</h1>
    <p>• We propose a lightweight two-stage neural network that is capable of learning a path connecting the start and goal SE(2) states directly from the environment. 
      A salient feature of our algorithm is its independence from tradi-tional sampling or search algorithms, which significantly enhances time efficiency and temporal stability. 
      Further-more, our network is adept at learning the paradigms of ground truth trajectories, thereby providing superior initial path for optimization. 
      This capability significantly reduces the back-end computational load and required optimization time.</p>
  
    <p>• We devise a distinctive trajectory formulation character-ized by a dual-layer, piece-wise polynomial architecture, 
      upon which we construct a differential flatness-based spatial-temporal joint trajectory optimization formulation. 
      This methodology is adept at not only facilitating the efficient generation of superior, collision-free trajectories but also at addressing the intrinsic ambiguities associated with flatness models, 
      thus augmenting the completeness. Furthermore, we meticulously derive the gradient propa-gation chain within this trajectory representation, 
      which empowers the proficient employment of gradient-based numerical optimization techniques.</p>

    <p>• We amalgamate the proposed front-end and back-end components to present a high-quality vehicle trajectory planner. Moreover, 
      we employ an iterative modal plan-ning algorithm to efficaciously mitigate the transitions between forward and reverse motions during vehicle ma-neuvering. 
      Additionally, we conduct a series of ablation studies to meticulously analyze the contribution of each component, 
      and comparative experiments are undertaken to validate the superiority of our algorithm.</p>
  
    <!-- 大标题 -->
    <h1>LEARNING-BASED PATH PLANNING</h1>

  <p>In this section, we introduce the learning-based path planner that leverages the performance of the network to directly output an initial path for optimization without the need for
  additional sampling or searching. Our path planner takes asinput the navigation start x^{p}_{i} and goal x^p_N, as well as the grid-based environment E of size H∗W, and directly outputs a path p = {xp1,...,xpi ,...,xpN} 
  serialized as a sequence of multiple state points. Here, the resolution of each grid is defined as rs. Moreover, each state point in the path is associated with the SE(2) state of the robot, 
  which includes its position and orientation angle. We firstly describe the network architecture, which initially estimates the distribution of each state point in the environmental space, 
  and then further reduces their uncertainty to determine the precise states of the points in the sequence. Then, we discuss the loss function used during training, which includes supervising the network’s 
  output with the ground truth and penalizing infeasible paths based on safety considerations and nonholonomic dynamic constraints.</p>
  
    
   <h2>A. Neural Network Architecture</h2>
    <div class="center">
      <img  width="840" height="540" src="https://github.com/mengze3/mengze3.github.io/blob/11d20e3403392d78c85e76b223132dde30c84f6a/_pages/model1.png?raw=true" alt="">
  </div>
    <p>Our network comprises three main components: the feature extraction layer, the global distribution layer, and the local correction layer, as illustrated in Fig. 1.Inpractice,directly localizing the position of a specific state point within the envi-ronment is challenging and labor-intensive. 
      Therefore, drawing inspiration from the R-CNN, our network follows a two-stage inference architecture. Initially, we uniformly partition E into Hl ∗ Wl region proposals, where each region proposal
      corresponds to a block of size H Hl ∗ W Wl in the environment. The center point of each region proposal is designated as an anchor point. Subsequently, we employ the global distribution layer to obtain the probability distribution of each state point with respect to the region proposals. 
      This step involves estimating the coarse spatial distribution of the point. Next, leveraging the local correction layer, we further obtain the accurate position of the state point based on the environmental features and the probability distribution obtained earlier. Intuitively, 
      our aim is to first approximately determine the region proposal in which each state point resides and then regress its positional offset relative to the anchor, thereby recovering the global position of the point.</p>
     <p>The input to our network consists of the environment E, the encoding of the start and goal states. Here, E is represented by a Euclidean Signed Distance Field (ESDF), 
       where each element represents the signed distance from obstacles at that location. Inspired by the work, we adopt a start-goal encoding strategy by highlighting patches of size c ∗ c on a tensor of size H ∗ W. Specifically, 
       we assign a value of -1 to the patch representing the start point and a value of 1 to the patch representing the goal point. The remaining positions in the tensor are set as 0. To fully represent the SE(2) space, 
       we introduce two additional H ∗ W tensors to capture the cosine and sine values of the robot orientations at the start and goal locations. 
       Specifically, one tensor represents the cosine values for the patches corresponding to the starting and target points, while the other tensor represents the sine values. Subsequently, 
       the aforementioned representations of the environment and the start and goal SE(2) states are concatenated to form a 4∗H∗W
      tensor, which serves as the input to the feature extraction layer of our model. The feature extraction layer consists of a Fully Convolutional Network (FCN) and a Transformer Encoder. 
       The FCN encodes path plan problem into a high-dimensional latent space, and the size of this latent feature is downsampled to d ∗ Hl ∗ Wl to match the shape of the probability distribution of region proposals. 
       Here, d represents the user-defined dimension of the latent features. Next, we apply the transformer module to further fuse the features within the latent space, without altering the shape of the feature tensor. 
       For brevity, we denote the output of the feature extraction layer as M. M is further inputted to the global distribution layer, resulting in the probability distribution map P over the region proposals for N points along the path, 
       with a size of N ∗Hl∗Wl. Here, we denote the probability of the i-th point belonging to the (j,k)-th region as ϱi,j,k. Moreover, it is evident that these probabilities should satisfy the principle of probability normalization:
      Hl−1Xj=0Wl−1Xk=0 ϱi,j,k = 1,∀i ∈ {1,...,N}. (1)</p>


   <h2>B. Loss Function</h2>
    <p>This section discusses the generation of a safe flight corridor (SFC) that the drone can follow without colliding with obstacles.
Previous methods in safe flight corridor generation often faced limitations, especially in complex environments where the drone must maneuver through tight spaces or avoid moving obstacles.
I utilize a novel approach to generate a more flexible and safer corridor, which adapts to the environment and the drone's dynamic constraints, ensuring a higher level of safety during flight.</p>


    <h1>Target recognition and localization</h1>
  
    <img src="https://github.com/mengze3/mengze3.github.io/blob/85ed4283c001b95214b7627a670d0f036b9bd630/_pages/old_detect.png?raw=true" alt="original detection">
    <p>  A formidable obstacle surfaced: the low accuracy of image recognition, attributed to inadequate data stemming from the diverse scenes encountered during UAV flights.</p>

      <div class="center">
        <img  width="480" height="360" src="https://github.com/mengze3/mengze3.github.io/blob/4b73183ed170615e62c442be03e2334af1856149/_pages/flur1.jpg?raw=true" alt="">
    </div>
    <p>    To tackle this challenge, we implemented innovative measures to augment our dataset. Recording videos enabled us to capture dynamic UAV images in flight, 
      expanding the range of perspectives and scenarios for more diverse training data. Additionally, we incorporated a jitter to simulate visual instability encountered in actual
      flights, enhancing the realism of our training data.</p>

    <img src="https://github.com/mengze3/mengze3.github.io/blob/629a88ce29542fd2cee39b2c2a2153936f48cf86/_pages/new_detect.png?raw=true" alt="improved detection">
    <table border="1">
    <tr>
      <th style="text-align: center;"></th>
      <th style="text-align: center;">original detection</th>
      <th style="text-align: center;">improved detection</th>
    </tr>
    <tr>
      <td style="text-align: center;">Precision(%)</td>
      <td style="text-align: center;">63</td>
      <td style="text-align: center;">98</td>
    </tr>
    <tr>
      <td style="text-align: center;">Recall(%)</td>
     <td style="text-align: center;">36</td>
      <td style="text-align: center;">97</td>
    </tr>
  </table>
    <p>    Through persistent efforts and optimization, our image recognition accuracy experienced a substantial boost, with recall rates increasing from 36% to 97% and precision surging from 68% to 98%.
      This remarkable improvement has significantly bolstered the robustness of our robot tracking. Particularly noteworthy is that when the confidence (threshold) is set at 0.5, 
      the system demonstrates exceptionally high reliability and efficiency.</p>


    <img src="https://github.com/mengze3/mengze3.github.io/blob/2a12037a0b6628c7cc9cf14e5cf4312e5e556653/_pages/yolo.png?raw=true" alt="improved detection">
    <p>The left side of the image shows examples of objects identified by the trained model, while the right side displays charts of various training metrics. 
      These charts include changes in the loss function, as well as variations in evaluation metrics such as precision, recall, and mean Average Precision (mAP).</p>
    <p>Loss Function: The train/box_loss, train/obj_loss, train/cls_loss, and corresponding validation set losses val/box_loss, val/obj_loss, 
      val/cls_loss all show a gradual decrease with increasing training epochs, indicating that the model is progressively improving, learning to better identify and classify objects.</p>
    <p>Precision and Recall: The metrics/precision and metrics/recall charts show that both these metrics are high, close to 1.0, 
      indicating that the model can correctly identify most true positives and rarely misclassifies negatives as positives.</p>
    <p>Mean Average Precision (mAP): The metrics/mAP_0.5 and metrics/mAP_0.5:0.95 represent the mAP at IoU (Intersection over Union) thresholds of 0.5 and between 0.5 to 0.95, respectively. 
      Both of these metrics also exhibit high values, especially the metrics/mAP_0.5 is near perfect, signifying that the model performs well across different IoU thresholds</p>
  
</body>
